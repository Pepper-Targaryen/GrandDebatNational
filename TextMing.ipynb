{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge T2 - Group 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content:\n",
    "1. Text stemming\n",
    "2. Clustering and interpretation\n",
    "3. Classification and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It takes one or two hours to finish the clustering with all the data,\n",
    "if you want to test the code,\n",
    "just take like the first 3000 or so ones.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import string\n",
    "import stop_words\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "To use the two packages of nltk, do \n",
    "nltk.download(\"stopwords\")\n",
    "and\n",
    "nltk.download('punkt')\n",
    "in the python console\n",
    "after you've installed nltk\n",
    "if you don't want certain word that appears but not filtered by me,\n",
    "add it in the list ['', '’', '``', '\\'\\'', '»', '...','«', 'nan', '--']\n",
    "in the 3rd line of the get_stop_words() function\n",
    "\"\"\"\n",
    "\n",
    "def get_stop_words():\n",
    "    custom_stop_words = set(stopwords.words('french') +\n",
    "                            list(string.punctuation) +\n",
    "                            ['', '’', '``', '\\'\\'', '»', '...','«', 'nan', '--'] +\n",
    "                            stop_words.get_stop_words('fr'))\n",
    "    return custom_stop_words\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    stemmer = FrenchStemmer()\n",
    "    words_temp = word_tokenize(text, language='french')\n",
    "    words_no_prefix = [f[2:] if f.startswith((\"l\\'\",\"d\\'\",\"j\\'\",\"n\\'\",\"c\\'\")) else f for f in words_temp]\n",
    "    words_no_prefix = [f[3:] if f.startswith((\"qu\\'\")) else f for f in words_no_prefix]\n",
    "    words_prefect = [stemmer.stem(word) for word in words_no_prefix if not word.isdigit()]\n",
    "    return words_prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"DEMOCRATIE_ET_CITOYENNETE.csv\", \"LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv\",\n",
    "            \"LA_TRANSITION_ECOLOGIQUE.csv\", \"ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv\"]\n",
    "file_name = file_names[0] # name of the file\n",
    "\n",
    "df = pd.read_csv(\"data/\" + file_name, low_memory=False)\n",
    "n_questions= df.shape[1] - 11 # 11 features are basic information\n",
    "n_answers = df.shape[0]\n",
    "\n",
    "#Use this of you only want to do one certain question\n",
    "\n",
    "'''\n",
    "answers = df[\"QUXVlc3Rpb246MTA3 - En qui faites-vous le plus confiance pour vous faire représenter dans la société et pourquoi ?\"]\n",
    "answers = answers.str.lower()\n",
    "answers = answers.values.tolist()\n",
    "answers = [x for x in answers if type(x) is str]\n",
    "'''\n",
    "\n",
    "#Use this if you want to combine the responses of all the questions\n",
    "answers = df.iloc[:,11:]\n",
    "answers = answers.astype(str)\n",
    "answers = answers.apply(\" \".join, axis =1)\n",
    "answers = answers.str.lower()\n",
    "answers = answers.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize our text\n",
    "custom_stop_words = get_stop_words()\n",
    "vectorizer = TfidfVectorizer(stop_words=custom_stop_words,\n",
    "                            tokenizer=tokenize,\n",
    "                            max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pepper/Documents/WorkSpace/python3environment/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['alor', 'aur', 'aurion', 'auron', 'auss', 'autr', 'avi', 'avion', 'avon', 'ayon', 'cec', 'cel', 'chaqu', 'comm', 'dan', 'dedan', 'dehor', 'devr', 'devrion', 'devron', 'droit', 'e', 'encor', 'euss', 'eussion', 'eûm', 'fair', 'forc', 'fuss', 'fussion', 'fûm', 'hor', 'just', 'mainten', 'moin', 'mêm', 'nomm', 'notr', 'parc', 'parol', 'person', 'san', 'ser', 'serion', 'seron', 'seul', 'somm', 'soyon', 'tand', 'tel', 'tres', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The X in the next line is the matrix transformed from all the text data,\n",
    "if you wants to split the data to train/text or just divide,\n",
    "split the X\n",
    "'''\n",
    "\n",
    "X = vectorizer.fit_transform(answers)\n",
    "words = vectorizer.get_feature_names()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : dan, franc, plus, droit, loi, an, san, citoyen, comm, cel, person, faut, fair, national, ser, vot, autr, vi, grand, mêm, non, devr, enfant, débat, polit\n",
      "1 : dan, plus, oui, franc, faut, fair, non, respect, pay, person, citoyen, autr, vot, cel, comm, polit, notr, bien, aid, mêm, gen, san, élus, immigr, vi\n",
      "2 : citoyen, dan, oui, plus, vot, élus, non, polit, représent, local, associ, fair, faut, élect, national, assembl, respect, déput, particip, mandat, comm, part, commun, pouvoir, autr\n",
      "3 : oui, non, vot, plus, chos, dan, respect, obligatoir, élus, citoyen, sais, proportionnel, déput, mair, fair, supprim, franc, blanc, référendum, immigr, person, travail, polit, comm, associ\n"
     ]
    }
   ],
   "source": [
    "#train the cluster\n",
    "kmeans = KMeans(n_clusters=4, n_init=20)\n",
    "kmeans.fit(X)\n",
    "\n",
    "#Display the clustering results\n",
    "common_words = kmeans.cluster_centers_.argsort()[:, -1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kmeans.labels_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pepper/Documents/WorkSpace/python3environment/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# labels from clustering\n",
    "# transform labels to onehot code\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "y_1hot = cat_encoder.fit_transform(y.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "y_train, y_test = train_test_split(y_1hot, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2 with Decision tree\n",
    "'''\n",
    "from sklearn import tree\n",
    "\n",
    "clf_DT = tree.DecisionTreeClassifier()\n",
    "clf_DT = clf_DT.fit(X_train, y_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7352643305651205"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_DT.score(X_test, y_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "STEP 2 with Naive Bayes\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(X, y)\n",
    "\n",
    "'''\n",
    "STEP 2 with KNN\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors = 8)\n",
    "neigh.fit(X, y)\n",
    "\n",
    "'''\n",
    "STEP 2 train report\n",
    "'''\n",
    "from sklearn import metrics\n",
    "\n",
    "y_predicted = neigh.predict(X)\n",
    "print(metrics.classification_report(y, y_predicted))\n",
    "\n",
    "#TODO\n",
    "#Use the STEP1 cluster and the STEP2 cluster to predict the same\n",
    "#block of data, use the result of the cluster as the correct one\n",
    "#track the performance. Or you can change the parameters as you wish\n",
    "\n",
    "#TODO IMPORTANT\n",
    "#observe the clustering result and try to find a meaningful representation\n",
    "#of each cluster like \"people with a negative view\", \"people who don't believe\n",
    "# democracy\", etc. Like this our report will be more meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
